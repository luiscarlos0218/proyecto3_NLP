{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GRQnxMzISE_"
   },
   "source": [
    "# Proyecto 03 - Procesamiento del Lenguaje Natural\n",
    "\n",
    "## Dataset: The Multilingual Amazon Reviews Corpus\n",
    "\n",
    "**Recuerda descargar el dataset de [aquí](https://github.com/kang205/SASRec). Es un archivo .zip que contiene tres documentos. Más información sobre el dataset [aquí](https://registry.opendata.aws/amazon-reviews-ml/). Es importante que tengas en cuenta la [licencia](https://docs.opendata.aws/amazon-reviews-ml/license.txt) de este dataset.**\n",
    "\n",
    "### Exploración de datos y Procesamiento del Lenguaje Natural\n",
    "\n",
    "Dedícale un buen tiempo a hacer un Análisis Exploratorio de Datos. Considera que hasta que no hayas aplicado las herramientas de Procesamiento del Lenguaje Natural vistas, será difícil completar este análisis. Elige preguntas que creas que puedas responder con este dataset. Por ejemplo, ¿qué palabras están asociadas a calificaciones positivas y qué palabras a calificaciones negativas?\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "Implementa un modelo que, dada la crítica de un producto, asigne la cantidad de estrellas correspondiente. **Para pensar**: ¿es un problema de Clasificación o de Regresión?\n",
    "\n",
    "1. Haz todas las transformaciones de datos que consideres necesarias. Justifica.\n",
    "1. Evalúa de forma apropiada sus resultados. Justifica la métrica elegida.\n",
    "1. Elige un modelo benchmark y compara tus resultados con este modelo.\n",
    "1. Optimiza los hiperparámetros de tu modelo.\n",
    "1. Intenta responder la pregunta: ¿Qué información está usando el modelo para predecir?\n",
    "\n",
    "**Recomendación:** si no te resulta conveniente trabajar en español con NLTK, te recomendamos que explores la librería [spaCy](https://spacy.io/).\n",
    "\n",
    "### Para pensar, investigar y, opcionalmente, implementar\n",
    "1. ¿Valdrá la pena convertir el problema de Machine Learning en un problema binario? Es decir, asignar únicamente las etiquetas Positiva y Negativa a cada crítica y hacer un modelo que, en lugar de predecir las estrellas, prediga esa etiqueta. Pensar en qué situación puede ser útil. ¿Esperas que el desempeño sea mejor o peor?\n",
    "1. ¿Hay algo que te gustaría investigar o probar?\n",
    "\n",
    "### **¡Tómate tiempo para investigar y leer mucho!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1GFwraSISFB"
   },
   "outputs": [],
   "source": [
    "# Herramientas de uso\n",
    "import itertools\n",
    "\n",
    "# Gemas del Infinito\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Adicionales\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "#Machine Learning\n",
    "\n",
    "\n",
    "#NLP\n",
    "import nltk\n",
    "\n",
    "# Configuración NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodología de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui irá un resumen de lo desarrollado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Espacio de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dataset Dev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev = pd.read_json('dataset_amazon/dataset_es_dev.json', lines= True)\n",
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dataset Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_json('dataset_amazon/dataset_es_train.json', lines= True)\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dataset Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_json('dataset_amazon/dataset_es_test.json', lines= True)\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploración inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de instancias en el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.countplot(data=dataset_dev, x='product_category',order = dataset_dev.product_category.value_counts().index)\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se encuentra que la principal categoria de reseñas correspontes a wireless, home, toy, sports y home_improvement. Ahora buscaremos dividir el dataset según el número de estrellas y ubicar qué categorias son populares a partir de la calificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploracion por calificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorias populares con 1 estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star = dataset_dev[dataset_dev.stars==1]\n",
    "one_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.countplot(data=one_star, x='product_category',order = one_star.product_category.value_counts().index,palette='rocket')\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorias populares con 2 estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_star = dataset_dev[dataset_dev.stars==2]\n",
    "two_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.countplot(data=two_star, x='product_category',order = two_star.product_category.value_counts().index,palette='rocket')\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorias populares con 3 estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_star = dataset_dev[dataset_dev.stars==3]\n",
    "three_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.countplot(data=three_star, x='product_category',order = three_star.product_category.value_counts().index,palette='rocket')\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorias populares con 4 estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_star = dataset_dev[dataset_dev.stars==4]\n",
    "four_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.countplot(data=four_star, x='product_category',order = four_star.product_category.value_counts().index,palette='rocket')\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorias populares con 5 estrella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star = dataset_dev[dataset_dev.stars==5]\n",
    "five_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.countplot(data=five_star, x='product_category',order = five_star.product_category.value_counts().index,palette='rocket')\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que sin distincion a la clasificacion, las categorias parecen seguir el mismo comportamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comportamiento nos indica que el numero de estrellas no esta muy relacionado a la categoria de un producto. Esto es, productos de x o y categoria no se distinguen por su alto/bajo calificacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analisis exploratorio de reviews y title reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion tokenizador de texto\n",
    "def tokenizer(data,columna):\n",
    "    data['text']=data[columna].apply(lambda x:x.lower())\n",
    "    data['text']=data['text'].apply(lambda x: nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(x))\n",
    "    flatten_text=[word for l in data.text.values for word in l]\n",
    "    return flatten_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Exploracion de title Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_processing(data,columna):\n",
    "    stopwd=nltk.corpus.stopwords.words('spanish')\n",
    "    flatten_text=tokenizer(data,columna)\n",
    "    content=[word for word in flatten_text if word not in stopwd]\n",
    "    print(f'El porcentaje de palabras a procesar equivale a: {len(content)/len(flatten_text) *100} %')                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos el porcentaje de palabras que tenemos para procesar \n",
    "words_processing(dataset_dev,'review_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_reviews = []\n",
    "calification = []\n",
    "for i in range(dataset_dev.shape[0]):\n",
    "    title = dataset_dev.iloc[i].review_title.lower()\n",
    "    star = dataset_dev.iloc[i].stars\n",
    "    title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "    title = [word for word in title if word not in stopwords]\n",
    "\n",
    "    title_reviews.append(title)\n",
    "    calification.append(star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_reviews = list(itertools.chain(*title_reviews))\n",
    "title_reviews[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tl = nltk.FreqDist(title_reviews)\n",
    "freq_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"W_Frequency\"])\n",
    "df_tl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl.sort_values('W_Frequency',ascending=False, inplace = True)\n",
    "df_tl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl.reset_index(drop = True, inplace=True)\n",
    "df_tl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.barplot(x  = df_tl.iloc[:30].Word, y = df_tl.iloc[:30].W_Frequency)\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan las palabrás más frecuentes en los títulos de los reviews. Aunque esta gráfica no nos permite diferenciar cuales se refieren a críticas buenas o malas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definira una funcion que nos permite realizar el tokenizado clasificando a partir del numero de estrellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_npl(dataset, stopwords, star):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        title = dataset.iloc[i].review_title.lower()\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        title = [word for word in title if word not in stopwords]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "        \n",
    "    title_reviews = list(itertools.chain(*title_reviews))\n",
    "    freq_tl = nltk.FreqDist(title_reviews)\n",
    "    df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"Frequency\"])\n",
    "    df_tl['Calification'] = star\n",
    "    df_tl.sort_values('Frequency',ascending=False, inplace = True)\n",
    "    df_tl.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    return df_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = token_npl(one_star, stopwords, 1)\n",
    "df_one.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two = token_npl(two_star, stopwords, 2)\n",
    "df_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_three = token_npl(three_star, stopwords, 3)\n",
    "df_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_three = token_npl(three_star, stopwords, 3)\n",
    "df_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_four = token_npl(four_star, stopwords, 4)\n",
    "df_four.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five = token_npl(five_star, stopwords, 5)\n",
    "df_five.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se encuentra que los titulos de los reviews no se encuentran palabras relevantes en relacion a la calificacion que el usuario le da al producto. Por el contrario, se repiten palabras independientemente de la calificacion. Por lo que esta columna de los datos puede darnos un panorama confuso al momento de entrenar el modelo.\n",
    "Sin embargo un analisis de ngramas pueden darnos alguna idea de las frases que se repiten comunmente en los comentarios buenos y malos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dividir el dataset en dos segun la calificacion que se dio, para ello definiremos bueno calificaciones mayores a 3 y malos 3 o menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = dataset_dev[dataset_dev.stars>=3]\n",
    "bad=dataset_dev[dataset_dev.stars<3]\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para dividir los titulos en ngramas \n",
    "from nltk.util import ngrams\n",
    "def ngramas(data,ngrama):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(data.shape[0]):\n",
    "        title = data.iloc[i].review_title.lower()\n",
    "        star = data.iloc[i].stars\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        title = [word for word in title if word not in stopwords]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "    #Separamos por palabras \n",
    "    title_reviews_flatten = [word for l in title_reviews for word in l]\n",
    "    #construimos los ngrams\n",
    "    ngramas=list(ngrams(title_reviews_flatten,ngrama))\n",
    "    fdist_n=nltk.FreqDist(ngramas)\n",
    "    print(f'Los 10 ngramas mas comunes son: \\n {fdist_n.most_common(10)}\\n')\n",
    "    print('Grafica de los ngramas mas comunes')\n",
    "    fdist_n.plot(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngramas para comentarios con calificación buena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Palabras a procesar\n",
    "words_processing(dataset_dev,'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramas(good,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigramas\n",
    "ngramas(good,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngramas para comentarios con calificacion mala "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrama\n",
    "ngramas(bad,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trigrama\n",
    "ngramas(bad,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el analisis de ngramas podemos ver frases comunes que en el lenguaje natural explican el porque de la calificación buena o mala de cada item. \n",
    "\n",
    "Ahora analizaremos el review body para descubrir colocaciones que nos ayuden a encontrar patrones del lenguaje en ambos datasets \"good\" y \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definamos una funcion para encontrar las coloaciones de dos o mas palabras \n",
    "import plotly.express as px\n",
    "def colocaciones(data):\n",
    "    review_bodies = []\n",
    "    calification = []\n",
    "    for i in range(data.shape[0]):\n",
    "        review = data.iloc[i].review_body.lower()\n",
    "        star = data.iloc[i].stars\n",
    "        review = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(review)\n",
    "        review = [word for word in review if word not in stopwords]\n",
    "\n",
    "        review_bodies.append(review)\n",
    "        calification.append(star)\n",
    "\n",
    "    #Separamos por palabras \n",
    "    reviews_flatten= [word for l in review_bodies for word in l]\n",
    "    #construimos los ngrams\n",
    "    trigramas=list(ngrams(reviews_flatten,3))\n",
    "    trigramas_df=pd.DataFrame()\n",
    "    trigramas_df['trigramas']=list(set(trigramas))\n",
    "    trigramas_df['word_1']=trigramas_df['trigramas'].apply(lambda x: x[0])\n",
    "    trigramas_df['word_2']=trigramas_df['trigramas'].apply(lambda x: x[1])\n",
    "    trigramas_df['word_3']=trigramas_df['trigramas'].apply(lambda x: x[2])\n",
    "    #conteo para sacar el PMI pointwise mutual information\n",
    "    fdist_trigram=nltk.FreqDist(trigramas)\n",
    "    fdist_palabra=nltk.FreqDist(reviews_flatten)\n",
    "\n",
    "    trigramas_df['frec_trigram']=trigramas_df['trigramas'].apply(lambda x:fdist_trigram[x])\n",
    "    trigramas_df['frec_word_1']=trigramas_df['word_1'].apply(lambda x:fdist_palabra[x])\n",
    "    trigramas_df['frec_word_2']=trigramas_df['word_2'].apply(lambda x:fdist_palabra[x])\n",
    "    trigramas_df['frec_word_3']=trigramas_df['word_3'].apply(lambda x:fdist_palabra[x])\n",
    "\n",
    "    #calcular el PMI metrica de colocacion \n",
    "    trigramas_df['PMI']=trigramas_df[['frec_trigram','frec_word_1','frec_word_2']].apply(lambda x:np.log2(x[0]/(x[1]*x[2])),axis=1)\n",
    "    \n",
    "    #ver graficamente las colocaciones\n",
    "    trigramas_df['log_frec_trigram']=trigramas_df['frec_trigram'].apply(lambda x: np.log2(x))\n",
    "    fig=px.scatter(x=trigramas_df.PMI , y=trigramas_df.log_frec_trigram ,color=trigramas_df.PMI+trigramas_df.log_frec_trigram , hover_name=trigramas_df.trigramas , width=600 , height=600, labels={'x':'PMI','y':'Frecuencia'} )\n",
    "    fig.show()\n",
    "    return trigramas_df.sort_values('PMI',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colocaciones(good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colocaciones(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ser comentarios sobre distintos productos de gente con diferente cultura no fue posible enciontrar colocaciones dicientes, sin embargo en el dataset de buena calificacion (good) se evidencia una que tiene un PMI cercanoia  cero y una frecuencia alta, la cual corresponde al trigrama de \"doy 5 estrellas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exploracion de Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_npl(dataset, stopwords, star):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        title = dataset.iloc[i].review_body.lower()\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        title = [word for word in title if word not in stopwords]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "        \n",
    "    title_reviews = list(itertools.chain(*title_reviews))\n",
    "    freq_tl = nltk.FreqDist(title_reviews)\n",
    "    df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"Frequency\"])\n",
    "    df_tl['Calification'] = star\n",
    "    df_tl.sort_values('Frequency',ascending=False, inplace = True)\n",
    "    df_tl.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    return df_tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Exploracion general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = token_npl(dataset_dev, stopwords, 1)\n",
    "df_r.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan que estas palabras, al igual que en el ejemplo con las tittle reviews, son muy generales y relacionadas al contexto del dataset (calidad, producto, si, precio, bien). Se modificaran las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtrar = ['bien','producto','si','buena','mas','bastante','buen','dos','hace','calidad','precio','queda','solo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_npl(dataset, stopwords, star):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        title = dataset.iloc[i].review_body.lower()\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        title = [word for word in title if word not in stopwords]\n",
    "        title = [word for word in title if word not in filtrar]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "        \n",
    "    title_reviews = list(itertools.chain(*title_reviews))\n",
    "    freq_tl = nltk.FreqDist(title_reviews)\n",
    "    df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"Frequency\"])\n",
    "    df_tl['Calification'] = star\n",
    "    df_tl.sort_values('Frequency',ascending=False, inplace = True)\n",
    "    df_tl.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    return df_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = token_npl(dataset_dev, stopwords, 1)\n",
    "del df_r['Calification']\n",
    "df_r.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Exploración a partir de la clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_one = token_npl(one_star, stopwords, 1)\n",
    "df_r_one.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_two = token_npl(two_star, stopwords, 2)\n",
    "df_r_two.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_three = token_npl(three_star, stopwords, 3)\n",
    "df_r_three.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_four = token_npl(four_star, stopwords, 4)\n",
    "df_r_four.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_five = token_npl(five_star, stopwords, 5)\n",
    "df_r_five.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = pd.concat([df_r_one, df_r_two, df_r_three, df_r_four, df_r_five])\n",
    "df_token.sort_values('Frequency',ascending=False, inplace = True)\n",
    "df_token.reset_index(drop = True, inplace=True)\n",
    "df_token.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.barplot(x  = df_token.iloc[:30].Word, y = df_token.iloc[:30].Frequency)\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaremos este filtro a la exploración de los titulos de los reviews para observar posibles cambios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_npl(dataset, stopwords, star):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        title = dataset.iloc[i].review_title.lower()\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        title = [word for word in title if word not in stopwords]\n",
    "        title = [word for word in title if word not in filtrar]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "        \n",
    "    title_reviews = list(itertools.chain(*title_reviews))\n",
    "    freq_tl = nltk.FreqDist(title_reviews)\n",
    "    df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"Frequency\"])\n",
    "    df_tl['Calification'] = star\n",
    "    df_tl.sort_values('Frequency',ascending=False, inplace = True)\n",
    "    df_tl.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    return df_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = token_npl(one_star, stopwords, 1)\n",
    "df_one.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two = token_npl(two_star, stopwords, 2)\n",
    "df_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_three = token_npl(three_star, stopwords, 3)\n",
    "df_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_four = token_npl(four_star, stopwords, 4)\n",
    "df_r_four.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five = token_npl(five_star, stopwords, 5)\n",
    "df_five.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que con la lista de palabras para filtrar si es posible observar algunas relaciones entre palabras clave en los reviews y title reviews y la calificación del usuario al producto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplicará una técnica de stemming al dataset final para observar comportamientos y relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.concat([dataset_dev.stars,dataset_dev.review_body,dataset_dev.review_title,dataset_dev.product_category],axis=1)\n",
    "dataset.dropna(axis=0,inplace=True)  # Si hay alguna nan, tiramos esa instancia\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que en nuestro tenemos reviews, los signos de exclamacion y pregunta no son relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Para Title Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos todos los titulos y le vamos aplicando la Normalizacion y luega el Stemming a cada uno\n",
    "titular_list=[]\n",
    "for titular in dataset.review_title:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras del titular\n",
    "    titular=nltk.word_tokenize(titular)\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    titular=[stemmer.stem(palabra) for palabra in titular]\n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular=\" \".join(titular)\n",
    "    \n",
    "    # Vamos armando una lista con todos los titulares\n",
    "    titular_list.append(titular)\n",
    "    \n",
    "dataset[\"title_review_nr\"] = titular_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Para Body Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos todos los titulos y le vamos aplicando la Normalizacion y luega el Stemming a cada uno\n",
    "titular_list=[]\n",
    "for titular in dataset.review_body:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras del titular\n",
    "    titular=nltk.word_tokenize(titular)\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    titular=[stemmer.stem(palabra) for palabra in titular]\n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular=\" \".join(titular)\n",
    "    \n",
    "    # Vamos armando una lista con todos los titulares\n",
    "    titular_list.append(titular)\n",
    "    \n",
    "dataset[\"body_review_nr\"] = titular_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['review_body']\n",
    "del dataset['review_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Dataset Normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Exploracion dataset normalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Title Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinimos la funcion, quitamos el comentario de stopwords y filtro para observar como se comporta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_npl(dataset, stopwords, star):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        title = dataset.iloc[i].title_review_nr.lower()\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        #title = [word for word in title if word not in stopwords]\n",
    "        #title = [word for word in title if word not in filtrar]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "        \n",
    "    title_reviews = list(itertools.chain(*title_reviews))\n",
    "    freq_tl = nltk.FreqDist(title_reviews)\n",
    "    df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"Frequency\"])\n",
    "    df_tl['Calification'] = star\n",
    "    df_tl.sort_values('Frequency',ascending=False, inplace = True)\n",
    "    df_tl.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    return df_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nr = token_npl(dataset, stopwords, 1)\n",
    "del df_nr['Calification']\n",
    "df_nr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploracion por calificacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_nr = dataset[dataset.stars==1]\n",
    "df_one_nr = token_npl(one_star_nr, stopwords, 1)\n",
    "df_one_nr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_star_nr = dataset[dataset.stars==2]\n",
    "df_two_nr = token_npl(two_star_nr, stopwords, 2)\n",
    "df_two_nr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_star_nr = dataset[dataset.stars==3]\n",
    "df_three_nr = token_npl(three_star_nr, stopwords, 3)\n",
    "df_three_nr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_star_nr = dataset[dataset.stars==4]\n",
    "df_four_nr = token_npl(four_star_nr, stopwords, 4)\n",
    "df_four_nr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_nr = dataset[dataset.stars==5]\n",
    "df_five_nr = token_npl(five_star_nr, stopwords, 5)\n",
    "df_five_nr.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan algunas palabras poco relevantes como \"producto\". Incluso es posible pensar que palabras como \"bien\", \"mal\", \"malo\" entregan poca informacion respecto a la clasificacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Body Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_npl(dataset, stopwords, star):\n",
    "    title_reviews = []\n",
    "    calification = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        title = dataset.iloc[i].body_review_nr.lower()\n",
    "        title = nltk.tokenize.RegexpTokenizer(\"[\\w]+\").tokenize(title)\n",
    "        #title = [word for word in title if word not in stopwords]\n",
    "        #title = [word for word in title if word not in filtrar]\n",
    "\n",
    "        title_reviews.append(title)\n",
    "        calification.append(star)\n",
    "        \n",
    "    title_reviews = list(itertools.chain(*title_reviews))\n",
    "    freq_tl = nltk.FreqDist(title_reviews)\n",
    "    df_tl = pd.DataFrame(list(freq_tl.items()), columns = [\"Word\",\"Frequency\"])\n",
    "    df_tl['Calification'] = star\n",
    "    df_tl.sort_values('Frequency',ascending=False, inplace = True)\n",
    "    df_tl.reset_index(drop = True, inplace=True)\n",
    "        \n",
    "    return df_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nr_bd = token_npl(dataset, stopwords, 1)\n",
    "del df_nr_bd['Calification']\n",
    "df_nr_bd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploracion por calificacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_nr_bd = dataset[dataset.stars==1]\n",
    "df_one_nr_bd = token_npl(one_star_nr_bd, stopwords, 1)\n",
    "df_one_nr_bd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_star_nr_bd = dataset[dataset.stars==2]\n",
    "df_two_nr_bd = token_npl(two_star_nr_bd, stopwords, 1)\n",
    "df_two_nr_bd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_star_nr_bd = dataset[dataset.stars==3]\n",
    "df_three_nr_bd = token_npl(three_star_nr_bd, stopwords, 1)\n",
    "df_three_nr_bd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_star_nr_bd = dataset[dataset.stars==4]\n",
    "df_four_nr_bd = token_npl(four_star_nr_bd, stopwords, 1)\n",
    "df_four_nr_bd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_nr_bd = dataset[dataset.stars==5]\n",
    "df_five_nr_bd = token_npl(five_star_nr_bd, stopwords, 1)\n",
    "df_five_nr_bd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras como \"bien\", \"produco\", \"calidad\" se siguen repitiendo en cada clasificacion de manera repetitiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Lemmatization para Title Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset_dev.review_title,dataset_dev.stars,dataset_dev.review_body],axis=1)\n",
    "dataset.dropna(axis=0,inplace=True)\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titular_list=[]\n",
    "for titular in dataset.review_title:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras\n",
    "    titular=nltk.word_tokenize(titular)\n",
    "    \n",
    "    # Aplicamos el Lemmatizer (Esto puede tardar un ratito)\n",
    "    frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in titular]\n",
    "    \n",
    "    # Sacamos las Stopwords\n",
    "    titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular=\" \".join(titular)\n",
    "    #dataset[\"titular_normalizado\"] = titular_list\n",
    "    titular_list.append(titular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"title_review_lm\"] = titular_list\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lemm = pd.concat([dataset.title_review_lm,dataset.stars],axis=1)\n",
    "#dataset_lemm.dropna(axis=0,inplace=True)  # Por si quedaron campos vacios\n",
    "dataset_lemm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titular_list=[]\n",
    "for titular in dataset.review_body:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras\n",
    "    titular=nltk.word_tokenize(titular)\n",
    "    \n",
    "    # Aplicamos el Lemmatizer (Esto puede tardar un ratito)\n",
    "    frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in titular]\n",
    "    \n",
    "    # Sacamos las Stopwords\n",
    "    titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular=\" \".join(titular)\n",
    "    #dataset[\"titular_normalizado\"] = titular_list\n",
    "    titular_list.append(titular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"body_review_lm\"] = titular_list\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lemm = pd.concat([dataset.body_review_lm,dataset.title_review_lm,dataset.stars],axis=1)\n",
    "dataset_lemm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lemm.dropna(axis=0,inplace=True)  # Por si quedaron campos vacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploracion por calificacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_lm = dataset_lemm[dataset_lemm.stars==1]\n",
    "df_one_lm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_lm = dataset_lemm[dataset_lemm.stars==2]\n",
    "df_two_lm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_three_lm = dataset_lemm[dataset_lemm.stars==3]\n",
    "df_three_lm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_four_lm = dataset_lemm[dataset_lemm.stars==4]\n",
    "df_four_lm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five_lm = dataset_lemm[dataset_lemm.stars==5]\n",
    "df_five_lm.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer paso paso par aun buen procesamiento vamos a asighnar etiquetas a nuestro conjunto de palabras, para ello usaremos la libreria Stanza de la universidad de Stanford la cual ya tiene unos algoritmos internos basados en redes neuronales para establecer estas etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalamos stanza\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza \n",
    "#descargamos las librerias de stanzas para español \n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le decimos a estanza en el pipeline los procesos que queremos que haga con nuestro texto \n",
    "nlp=stanza.Pipeline('es',processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(data,columna):\n",
    "    text_list=tokenizer(dataset_dev,columna)\n",
    "    text_str=\" \".join(text_list)\n",
    "    text_tagged=nlp(review_titles_str)\n",
    "    text_tagged=[(word.text,word.pos) for sentence in text_tagged.sentences for word in sentence.words]\n",
    "    return text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_titles_tagged=tagger(dataset_dev,'review_title')\n",
    "text_titles_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bodies_tagged=tagger(dataset_dev,'review_body')\n",
    "text_bodies_tagged"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS_Proyecto_03_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
